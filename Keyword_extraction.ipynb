{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dea83311",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f1593e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7236</th>\n",
       "      <td>994</td>\n",
       "      <td>1994</td>\n",
       "      <td>Single Transistor Learning Synapses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>994-single-transistor-learning-synapses.pdf</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Single Transistor Learning Synapses\\n\\nPaul Ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7237</th>\n",
       "      <td>996</td>\n",
       "      <td>1994</td>\n",
       "      <td>Bias, Variance and the Combination of Least Sq...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>996-bias-variance-and-the-combination-of-least...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bias, Variance and the Combination of\\nLeast S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7238</th>\n",
       "      <td>997</td>\n",
       "      <td>1994</td>\n",
       "      <td>A Real Time Clustering CMOS Neural Engine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>997-a-real-time-clustering-cmos-neural-engine.pdf</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>A Real Time Clustering CMOS\\nNeural Engine\\nT....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7239</th>\n",
       "      <td>998</td>\n",
       "      <td>1994</td>\n",
       "      <td>Learning direction in global motion: two class...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>998-learning-direction-in-global-motion-two-cl...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Learning direction in global motion: two\\nclas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7240</th>\n",
       "      <td>999</td>\n",
       "      <td>1994</td>\n",
       "      <td>Correlation and Interpolation Networks for Rea...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>999-correlation-and-interpolation-networks-for...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Correlation and Interpolation Networks for\\nRe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7241 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  year                                              title  \\\n",
       "0        1  1987  Self-Organization of Associative Database and ...   \n",
       "1       10  1987  A Mean Field Theory of Layer IV of Visual Cort...   \n",
       "2      100  1988  Storing Covariance by the Associative Long-Ter...   \n",
       "3     1000  1994  Bayesian Query Construction for Neural Network...   \n",
       "4     1001  1994  Neural Network Ensembles, Cross Validation, an...   \n",
       "...    ...   ...                                                ...   \n",
       "7236   994  1994                Single Transistor Learning Synapses   \n",
       "7237   996  1994  Bias, Variance and the Combination of Least Sq...   \n",
       "7238   997  1994          A Real Time Clustering CMOS Neural Engine   \n",
       "7239   998  1994  Learning direction in global motion: two class...   \n",
       "7240   999  1994  Correlation and Interpolation Networks for Rea...   \n",
       "\n",
       "     event_type                                           pdf_name  \\\n",
       "0           NaN  1-self-organization-of-associative-database-an...   \n",
       "1           NaN  10-a-mean-field-theory-of-layer-iv-of-visual-c...   \n",
       "2           NaN  100-storing-covariance-by-the-associative-long...   \n",
       "3           NaN  1000-bayesian-query-construction-for-neural-ne...   \n",
       "4           NaN  1001-neural-network-ensembles-cross-validation...   \n",
       "...         ...                                                ...   \n",
       "7236        NaN        994-single-transistor-learning-synapses.pdf   \n",
       "7237        NaN  996-bias-variance-and-the-combination-of-least...   \n",
       "7238        NaN  997-a-real-time-clustering-cmos-neural-engine.pdf   \n",
       "7239        NaN  998-learning-direction-in-global-motion-two-cl...   \n",
       "7240        NaN  999-correlation-and-interpolation-networks-for...   \n",
       "\n",
       "              abstract                                         paper_text  \n",
       "0     Abstract Missing  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1     Abstract Missing  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "2     Abstract Missing  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "3     Abstract Missing  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "4     Abstract Missing  Neural Network Ensembles, Cross\\nValidation, a...  \n",
       "...                ...                                                ...  \n",
       "7236  Abstract Missing  Single Transistor Learning Synapses\\n\\nPaul Ha...  \n",
       "7237  Abstract Missing  Bias, Variance and the Combination of\\nLeast S...  \n",
       "7238  Abstract Missing  A Real Time Clustering CMOS\\nNeural Engine\\nT....  \n",
       "7239  Abstract Missing  Learning direction in global motion: two\\nclas...  \n",
       "7240  Abstract Missing  Correlation and Interpolation Networks for\\nRe...  \n",
       "\n",
       "[7241 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"C://Users//Ansh Jhoshi//Downloads//keyword_extraction//papers.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1596d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values\n",
      "id      7241\n",
      "year      31\n",
      "title      7241\n",
      "event_type      3\n",
      "pdf_name      7241\n",
      "abstract      3923\n",
      "paper_text      7237\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of unique values\")\n",
    "for col in df.columns:\n",
    "    print(col, \"    \", df[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7a8f739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values of particular columns\n",
      "\n",
      "Poster       2146\n",
      "Spotlight     181\n",
      "Oral           95\n",
      "Name: event_type, dtype: int64\n",
      "4819\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values of particular columns\\n\")\n",
    "print(df[\"event_type\"].value_counts())\n",
    "print(df[\"event_type\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10ef0e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values of particular columns\n",
      "\n",
      "2017    679\n",
      "2016    569\n",
      "2014    411\n",
      "2015    403\n",
      "2012    368\n",
      "2013    360\n",
      "2011    306\n",
      "2010    292\n",
      "2009    262\n",
      "2008    250\n",
      "2007    217\n",
      "2002    207\n",
      "2004    207\n",
      "2005    207\n",
      "2006    204\n",
      "2003    198\n",
      "2001    197\n",
      "1993    158\n",
      "2000    152\n",
      "1996    152\n",
      "1995    152\n",
      "1998    151\n",
      "1999    150\n",
      "1997    150\n",
      "1991    144\n",
      "1990    143\n",
      "1994    140\n",
      "1992    127\n",
      "1989    101\n",
      "1988     94\n",
      "1987     90\n",
      "Name: year, dtype: int64\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique values of particular columns\\n\")\n",
    "print(df[\"year\"].value_counts())\n",
    "print(df[\"year\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7424c1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "new_words = [\"fig\", \"figure\", \"image\", \"sample\", \"using\",\n",
    "            \"show\", \"result\", \"large\", \"also\",\n",
    "            \"one\", \"two\", \"three\", \"four\", \"five\",\n",
    "            \"six\", \"seven\", \"eight\", \"nine\", \"zero\"]\n",
    "\n",
    "stop_words = list(stop_words.union(new_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b0636db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       self organization associative database applica...\n",
       "1       mean field theory layer visual cortex applicat...\n",
       "2       storing covariance associative long term poten...\n",
       "3       bayesian query construction neural network mod...\n",
       "4       neural network ensemble cross validation activ...\n",
       "                              ...                        \n",
       "7236    single transistor learning synapsis paul hasle...\n",
       "7237    bias variance combination least square estimat...\n",
       "7238    real time clustering cmos neural engine serran...\n",
       "7239    learning direction global motion class psychop...\n",
       "7240    correlation interpolation network real time ex...\n",
       "Name: paper_text, Length: 7241, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaning(text):\n",
    "    \n",
    "    #convert everything to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    # lt gt - less than greater than\n",
    "    text = re.sub(\"&lt;/?.*?&gt;\" , \"&lt;&gt;\", text)\n",
    "    \n",
    "    #remove special characters and digits\n",
    "    #\\d - digit |(or) \\W - not a word character\n",
    "    # capital for inverse\n",
    "    #() is for checking groups\n",
    "    text = re.sub(\"(\\\\d|\\\\W)\",\" \", text)\n",
    "    \n",
    "    #covert to list from string, by splitting it\n",
    "    text = text.split()\n",
    "    \n",
    "    #remove stopwords\n",
    "    text = [words for words in text if words not in stop_words]\n",
    "    \n",
    "    #remove words less than 3 letters\n",
    "    text = [words for words in text if len(words) >= 3]\n",
    "    \n",
    "    #lemmatize - get the root word\n",
    "    #trim words to their root words\n",
    "    lemma = WordNetLemmatizer()\n",
    "    \n",
    "    text = [lemma.lemmatize(word) for word in text]\n",
    "    \n",
    "    return ' '.join(text)\n",
    "\n",
    "\n",
    "docs = df['paper_text'].apply(lambda x:cleaning(x))\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "52797f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43e7da68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7241x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 2701864 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tfidf - convert text to feature vectors\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_df = 0.95,        #ignore elements that appear in 95% documents\n",
    "                    max_features = 1000,  #size of vocabulary\n",
    "                    ngram_range = (1,2))   #vocabulary contains single words, bigrams, trigrams\n",
    "\n",
    "\n",
    "word_counter = cv.fit_transform(docs)\n",
    "\n",
    "word_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64284358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "df = pd.DataFrame(record)\n",
    " \n",
    "# Creating a dataframe with 50%\n",
    "# values of original dataframe\n",
    "part_50 = df.sample(frac = 0.5)\n",
    " \n",
    "# Creating dataframe with\n",
    "# rest of the 50% values\n",
    "rest_part_50 = df.drop(part_50.index)\n",
    "\n",
    "This is the way to divide the dataframe into 2 parts\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "model = TfidfTransformer(smooth_idf = True, use_idf = True)\n",
    "model.fit(word_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54bef03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_it(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True) \n",
    "\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "# get feature names\n",
    "feature_names=cv.get_feature_names()\n",
    "\n",
    "\n",
    "\n",
    "def get_keywords(idx, docs):\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=model.transform(cv.transform([docs[idx]]))\n",
    "\n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_it(tf_idf_vector.tocoo())\n",
    "\n",
    "    #extract only the top n; n here is 10\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "\n",
    "\n",
    "idx = 941\n",
    "keywords = get_keywords(idx, docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "934372ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Title=====\n",
      "Algorithms for Non-negative Matrix Factorization\n",
      "\n",
      "=====Abstract=====\n",
      "Non-negative matrix factorization (NMF) has previously been shown to \r\n",
      "be a useful decomposition for multivariate data. Two different multi- \r\n",
      "plicative algorithms for NMF are analyzed. They differ only slightly in \r\n",
      "the multiplicative factor used in the update rules. One algorithm can be \r\n",
      "shown to minimize the conventional least squares error while the other \r\n",
      "minimizes the generalized Kullback-Leibler divergence. The monotonic \r\n",
      "convergence of both algorithms can be proven using an auxiliary func- \r\n",
      "tion analogous to that used for proving convergence of the Expectation- \r\n",
      "Maximization algorithm. The algorithms can also be interpreted as diag- \r\n",
      "onally rescaled gradient descent, where the rescaling factor is optimally \r\n",
      "chosen to ensure convergence. \n",
      "\n",
      "===Keywords===\n",
      "update 0.45\n",
      "rule 0.302\n",
      "matrix 0.258\n",
      "factorization 0.236\n",
      "theorem 0.176\n",
      "gradient 0.174\n",
      "divergence 0.168\n",
      "negative 0.162\n",
      "factor 0.158\n",
      "function 0.157\n"
     ]
    }
   ],
   "source": [
    "def print_results(idx,keywords, df):\n",
    "    # now print the results\n",
    "    print(\"\\n=====Title=====\")\n",
    "    print(df['title'][idx])\n",
    "    print(\"\\n=====Abstract=====\")\n",
    "    print(df['abstract'][idx])\n",
    "    print(\"\\n===Keywords===\")\n",
    "    for k in keywords:\n",
    "        print(k,keywords[k])\n",
    "\n",
    "\n",
    "print_results(idx,keywords, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d5be1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
